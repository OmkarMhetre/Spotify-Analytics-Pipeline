{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2197537-2deb-4edf-b8f7-0873495468a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.utils import AnalysisException\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "jdbc_hostname = \"spotify-db.c7cmw2844uny.ap-south-1.rds.amazonaws.com\"\n",
    "jdbc_port = 3306\n",
    "database_name = \"spotify\"\n",
    "\n",
    "jdbc_url = f\"jdbc:mysql://{jdbc_hostname}:{jdbc_port}/{database_name}?useSSL=false\"\n",
    "\n",
    "username = \"admin\"        \n",
    "password = \"DjOmkar8856\"\n",
    "\n",
    "catalog_name = \"spotify\"\n",
    "schema_name = \"bronze_schema\"\n",
    "\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog_name}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog_name}.{schema_name}\")\n",
    "\n",
    "\n",
    "tables_to_load = [\n",
    "    \"DimArtist\",\n",
    "    \"DimDate\",\n",
    "    \"DimTrack\",\n",
    "    \"DimUser\",\n",
    "    \"FactStream\"\n",
    "]\n",
    "\n",
    "primary_keys = {\n",
    "    \"DimArtist\": \"artist_id\",\n",
    "    \"DimDate\": \"date_key\",\n",
    "    \"DimTrack\": \"track_id\",\n",
    "    \"DimUser\": \"user_id\",\n",
    "    \"FactStream\": \"stream_id\"\n",
    "}\n",
    "\n",
    "incremental_cols = {\n",
    "    \"DimArtist\": \"updated_at\",\n",
    "    \"DimDate\": None,           \n",
    "    \"DimTrack\": \"updated_at\",\n",
    "    \"DimUser\": \"updated_at\",\n",
    "    \"FactStream\": \"stream_timestamp\"\n",
    "}\n",
    "\n",
    "def bronze_etl(source_table: str, primary_key: str, incremental_col: str = None):\n",
    "\n",
    "    target_table = f\"{catalog_name}.{schema_name}.{source_table.lower()}\"\n",
    "    \n",
    "    table_exists = spark.catalog.tableExists(target_table)\n",
    "    print(f\"\\nProcessing table: {source_table} â†’ {target_table}\")\n",
    "    print(f\"Bronze table exists: {table_exists}\")\n",
    "    \n",
    "\n",
    "    if (not table_exists) or (incremental_col is None):\n",
    "        print(f\"Running FULL LOAD for {source_table}...\")\n",
    "        df = (\n",
    "            spark.read.format(\"jdbc\")\n",
    "            .option(\"url\", jdbc_url)\n",
    "            .option(\"dbtable\", source_table)\n",
    "            .option(\"user\", username)\n",
    "            .option(\"password\", password)\n",
    "            .load()\n",
    "        )\n",
    "\n",
    "        df = df.withColumn(\"ingestion_ts\", current_timestamp())\n",
    "        \n",
    "        df.write.format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .saveAsTable(target_table)\n",
    "        \n",
    "        print(f\"Full load completed for {source_table}.\")\n",
    "        df.show(5)\n",
    "    \n",
    "    else:\n",
    "        print(f\"Running INCREMENTAL LOAD for {source_table}...\")\n",
    "        \n",
    "    \n",
    "        last_ts = spark.sql(f\"\"\"\n",
    "            SELECT COALESCE(MAX({incremental_col}), '1900-01-01')\n",
    "            FROM {target_table}\n",
    "        \"\"\").collect()[0][0]\n",
    "        \n",
    "        print(f\"Last processed {incremental_col}: {last_ts}\")\n",
    "        \n",
    " \n",
    "        incr_query = f\"\"\"\n",
    "        (SELECT *\n",
    "         FROM {source_table}\n",
    "         WHERE {incremental_col} > '{last_ts}') AS inc\n",
    "        \"\"\"\n",
    "        incr_df = (\n",
    "            spark.read.format(\"jdbc\")\n",
    "            .option(\"url\", jdbc_url)\n",
    "            .option(\"dbtable\", incr_query)\n",
    "            .option(\"user\", username)\n",
    "            .option(\"password\", password)\n",
    "            .load()\n",
    "        )\n",
    "        \n",
    "        incr_count = incr_df.limit(1).count()\n",
    "        if incr_count == 0:\n",
    "            print(f\"No new records for {source_table}. Skipping incremental load.\")\n",
    "            return\n",
    "        \n",
    "        incr_df = incr_df.withColumn(\"ingestion_ts\", current_timestamp())\n",
    "        \n",
    "        delta_table = DeltaTable.forName(spark, target_table)\n",
    "        (\n",
    "            delta_table.alias(\"t\")\n",
    "            .merge(incr_df.alias(\"s\"), f\"t.{primary_key} = s.{primary_key}\")\n",
    "            .whenMatchedUpdateAll()\n",
    "            .whenNotMatchedInsertAll()\n",
    "            .execute()\n",
    "        )\n",
    "        \n",
    "        print(f\"Incremental merge completed for {source_table}.\")\n",
    "        incr_df.show(5)\n",
    "\n",
    "\n",
    "for table in tables_to_load:\n",
    "    bronze_etl(\n",
    "        source_table=table,\n",
    "        primary_key=primary_keys[table],\n",
    "        incremental_col=incremental_cols[table]\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
